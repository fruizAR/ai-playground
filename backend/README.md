# Backend - OpenAI Orchestration API

API de orquestaciÃ³n entre aplicaciones Angular y servicios OpenAI usando FastAPI.

## ğŸš€ CaracterÃ­sticas

- **Endpoint `/ask` (POST)**: EnvÃ­a solicitudes a OpenAI con soporte de streaming
- **Endpoint `/status` (GET)**: Obtiene el estado del servicio y conexiÃ³n con OpenAI
- **Endpoint `/logs` (GET)**: Accede a los logs de interacciones
- **Streaming SSE**: Respuestas en tiempo real usando Server-Sent Events
- **CORS configurado**: Listo para conectar con aplicaciones Angular
- **Logging avanzado**: Usando Loguru para mejor trazabilidad

## ğŸ“‹ Requisitos

- Python 3.8+
- API Key de OpenAI

## ğŸ”§ InstalaciÃ³n

1. **Crear entorno virtual:**

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

2. **Instalar dependencias:**

```bash
pip install -r requirements.txt
```

3. **Configurar variables de entorno:**

Copia el archivo `.env.example` a `.env` y configura tu API Key de OpenAI:

```bash
cp .env.example .env
```

Edita `.env` y aÃ±ade tu API Key:

```
OPENAI_API_KEY=tu_api_key_aqui
OPENAI_MODEL=gpt-4-turbo-preview
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
```

## â–¶ï¸ EjecuciÃ³n

**Modo desarrollo (con hot reload):**

```bash
python run.py
```

O usando uvicorn directamente:

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Modo producciÃ³n:**

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

## ğŸ“š DocumentaciÃ³n API

Una vez iniciado el servidor, accede a:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ”Œ Endpoints

### POST /ask

EnvÃ­a un prompt a OpenAI y recibe la respuesta.

**Request Body:**

```json
{
  "prompt": "Explica quÃ© es FastAPI",
  "temperature": 0.7,
  "max_tokens": 1000,
  "stream": true
}
```

**Response (streaming):**

- Content-Type: `text/event-stream`
- Formato SSE con chunks de texto

**Response (no-streaming):**

```json
{
  "response": "FastAPI es un framework moderno...",
  "tokens_used": 150
}
```

### GET /status

Obtiene el estado del servicio.

**Response:**

```json
{
  "status": "running",
  "version": "1.0.0",
  "openai_connected": true
}
```

### GET /logs

Obtiene los logs de las interacciones.

**Query Parameters:**

- `limit` (opcional): NÃºmero de logs a retornar (default: 100, max: 1000)
- `level` (opcional): Filtrar por nivel (INFO, ERROR, WARNING)

**Response:**

```json
[
  {
    "timestamp": "2026-01-04T12:00:00",
    "level": "INFO",
    "message": "Generando respuesta para prompt...",
    "metadata": null
  }
]
```

## ğŸ—ï¸ Estructura del Proyecto

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ config.py            # ConfiguraciÃ³n y settings
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py       # Modelos Pydantic
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ openai_router.py # Endpoint /ask
â”‚   â”‚   â”œâ”€â”€ status_router.py # Endpoint /status
â”‚   â”‚   â””â”€â”€ logs_router.py   # Endpoint /logs
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ openai_service.py # LÃ³gica de OpenAI
â”œâ”€â”€ logs/                    # Directorio de logs
â”œâ”€â”€ .env.example            # Ejemplo de variables de entorno
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt        # Dependencias
â”œâ”€â”€ run.py                 # Script de inicio
â””â”€â”€ README.md
```

## ğŸ”’ Seguridad

- Las API Keys se almacenan en variables de entorno
- CORS configurado para orÃ­genes especÃ­ficos
- No se exponen claves en logs
- ValidaciÃ³n de entrada con Pydantic

## ğŸ§ª Pruebas

**Probar con curl:**

```bash
# Probar /status
curl http://localhost:8000/status

# Probar /ask (no-streaming)
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hola", "stream": false}'

# Probar /ask (streaming)
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hola", "stream": true}'

# Probar /logs
curl http://localhost:8000/logs?limit=10
```

## ğŸ“ Notas

- El streaming usa Server-Sent Events (SSE) para compatibilidad con Angular
- Los logs se almacenan en memoria (para producciÃ³n, usar base de datos)
- CORS estÃ¡ configurado para `localhost:4200` y `localhost:4201` (puertos tÃ­picos de Angular)
- En producciÃ³n, usar un servidor proxy inverso (nginx) y HTTPS

## ğŸš€ PrÃ³ximos Pasos

- Implementar autenticaciÃ³n JWT
- AÃ±adir rate limiting
- Persistir logs en base de datos
- AÃ±adir mÃ©tricas con Prometheus
- Implementar LangGraph para flujos complejos
